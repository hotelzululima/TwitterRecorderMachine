#!/usr/bin/env python
# encoding: utf-8
#DESC: download tweets from a given screen name
import tweepy, csv, sys, os, re
from time import gmtime, strftime
from simpleconfigparser import simpleconfigparser

config = simpleconfigparser()
config.read(os.environ['HOME'] +'/.twitter')
 
def get_all_tweets(screen_name,thelast):
	#Twitter only allows access to a users most recent 3240 tweets with this method
	
	#authorize twitter, initialize tweepy
	auth = tweepy.OAuthHandler(config.API.consumer_key, config.API.consumer_secret)
	auth.set_access_token(config.API.access_token_key, config.API.access_token_secret)
	api = tweepy.API(auth)

	#log details on user being checked
        y = api.get_user(screen_name)
        if y:
            userlog = open(os.environ['HOME'] + "/Dropbox/userlog.csv",'a')
            focnt = str(y.followers_count)
            frcnt = str(y.friends_count)
            statc = str(y.statuses_count)
            userlog.write(strftime("%Y-%m-%d %H:%M:%S", gmtime()) + "," + y.screen_name + "," + y.id_str + "," +  frcnt + "," + focnt + "," + statc + "," +  str(y.created_at) + "\n")
            #print(strftime("%Y-%m-%d %H:%M:%S", gmtime()) + "," + y.screen_name + "," + y.id_str + "," +  frcnt + "," + focnt + "," + statc + "," +  str(y.created_at))
            userlog.close()	
	#initialize a list to hold all the tweepy Tweets
	alltweets = []	
	
	#make initial request for most recent tweets. 200 is the maximum allowed count,
	#but 20 is a time proven choice that balances the need to be gentle with the API
	#while being efficient about capturing new content
	new_tweets = api.user_timeline(screen_name = screen_name,count=20)
	
        found = 0
        for tweet in new_tweets:
            if str(tweet.id) == thelast:
              found = 1
        alltweets.extend(new_tweets)
        oldest = alltweets[-1].id - 1
	while len(new_tweets) > 0 and found < 1:
		new_tweets = api.user_timeline(screen_name = screen_name,count=200,max_id=oldest)
                for tweet in new_tweets:
                   if str(tweet.id) == thelast:
                       found = 1
		alltweets.extend(new_tweets)
		oldest = alltweets[-1].id - 1

	tw = open(sys.argv[1],'w')	
        ra = open(sys.argv[1] + "-raw.txt",'w')
        um = open(sys.argv[1] + "-mentions",'w')
        sa = open(sys.argv[1] + "-devtype.csv",'w')
        for tweet in alltweets:
            if str(tweet.id) == thelast:
               tw.close()
               um.close()
               sa.close()
               sys.exit()
            s = tweet.text.encode('utf-8')
            #s = s.replace('\n', ' ')
            devtype = tweet.source.replace(' ', '')
            tw.write(str(tweet.id) + " " + str(tweet.created_at) + " UTC <" + user + "> " + s +" source::" + devtype.encode('utf-8') +  "\n")

            # there is probably a MUCH cleaner way to do this
	    # raw files are stripped of URLs to facilitate stylometry using JGAAP
            raws = re.sub("http:\/\/t.co\/[0-9a-zA-Z]{1,10}", "",s)
            raws = re.sub("https:\/\/t.co\/[0-9a-zA-Z]{1,10}", "",raws)
            raws = re.sub("http:\/\/yfrog.com\/[0-9a-zA-Z]{1,10}", "",raws) + "\n"
            raws = re.sub(">","",raws)
            raws = re.sub("<","",raws)
            raws = re.sub("&","",raws)

            rawwords = raws.split(" ")
            if rawwords[0] == "RT":
                # we're ignoring RTs
                nothing = 0
            else:
                sa.write(user + "," + devtype.encode('utf-8') + "," + str(tweet.created_at) + "\n")
                for forp in tweet.entities['user_mentions']:
                    um.write(sys.argv[1] + "," + forp['screen_name'] + "\n")
                    raws = re.sub("@" + forp['screen_name'], " ", raws)

                ra.write(raws)
            # end zap

            
	pass
 
 
if __name__ == '__main__':
    user = sys.argv[1]
    last = "999999999999999999"
    if len(sys.argv) > 2:
        last = sys.argv[2]
    get_all_tweets(user,last)
